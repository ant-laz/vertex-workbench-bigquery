{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08133e5-25d7-45a4-9a3e-a52e39f3d5e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36c9b9-43ff-4b26-96ae-b84798a480a3",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "This notebook offers demonstrations of different methods of querying data that is stored in BigQuery from within the JupyterLab interface of your Vertex AI Workbench instance.\n",
    "\n",
    "Confused ? Let's define each of these terms in case they're new to you\n",
    "\n",
    " * **BigQuery** BigQuery is a fully managed, AI-ready data platform that helps you manage and analyze your data with built-in features like machine learning, search, geospatial analysis, and business intelligence. BigQuery's serverless architecture lets you use languages like SQL and Python to answer your organization's biggest questions with zero infrastructure management.\n",
    " \n",
    " * **JupyterLab**  JupyterLab is a highly extensible, feature-rich notebook authoring application and editing environment, and is a part of Project Jupyter, a large umbrella project centered around the goal of providing tools (and standards) for interactive computing with computational notebooks.\n",
    " \n",
    " * **Vertex AI Workbench instance** Vertex AI Workbench instances are Jupyter notebook-based development environments on Google Cloud for the entire data science workflow. Vertex AI Workbench instances are prepackaged with JupyterLab. Vertex AI Workbench instances have integrations and features can make it easier to access your data, process data faster, schedule notebook runs, and more.\n",
    " \n",
    "For most of these methods, we will use a **Python JupyterLab Kernel**. In the Jupyter architecture, kernels are separate processes started by the server that run your code in different programming languages and environments.  We will use the IPython Jupyter Kernel that comes prepackaged with Vertex AI Workbench Instances. This will allow us to execute Python code in this notebook interactively.\n",
    "\n",
    "Reference: \n",
    "\n",
    "https://cloud.google.com/bigquery/docs/introduction\n",
    "\n",
    "https://jupyterlab.readthedocs.io/en/latest/user/interface.html\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/workbench/instances/introduction\n",
    "\n",
    "https://jupyterlab.readthedocs.io/en/latest/user/documents_kernels.html#kernel-backed-documents\n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab43f52-c2fb-461b-99a2-15f2b26b3b48",
   "metadata": {},
   "source": [
    "# Method 1 : Using the BigQuery Pane in Vertex AI workbnench\n",
    "\n",
    "This is Method 1 of querying data that is stored in BigQuery from within the JupyterLab interface of your Vertex AI Workbench instance.\n",
    "\n",
    "BigQuery is integrated into the the JupyterLab interface of your Vertex AI Workbench instance.\n",
    "\n",
    "Through this integration you can use a BigQuery pane that lists available projects and datasets. \n",
    "\n",
    "Let's use this BigQuery pane now to explore some BigQuery public datasets. \n",
    "\n",
    " * select project \"bigquery-public-data\"\n",
    "   * select datatset \"google_trends\"\n",
    "     * select table \"international_top_terms\"\n",
    "       * click \"preview\" to see a sample of rows from this table\n",
    "       * click \"query table\" to open the Stand-alone BigQuery query editor opens as a separate tab in JupyterLab.\n",
    "\n",
    "Reference : \n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/workbench/instances/bigquery#browse_resources\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/workbench/instances/bigquery#stand-alone\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73469eb3-379b-40a0-8e48-fe135a074578",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Method 2 : Using the %%bigquery magic command\n",
    "\n",
    "This is Method 2 of querying data that is stored in BigQuery from within the JupyterLab interface of your Vertex AI Workbench instance.\n",
    "\n",
    "For this method, it is required to use the IPython Jupyter Kernel that comes prepackaged with Vertex AI Workbench Instances. Please select & connect to it now.\n",
    "\n",
    "The IPython Jupyter Kernel has a feature called \"cell magics\" which are handy commands built into the IPython kernel that make it easy to perform particular tasks.\n",
    "\n",
    "BigQuery has developed \"cell magics\" to make it easy to execute SQL queries. There are two BigQuery \"cell magics\" \n",
    "\n",
    "1. %%bigquery\n",
    "\n",
    "  *  Behind the scenes, the %%bigquery magic command uses the BigQuery client library for Python to run the given query\n",
    "  *  Then convert the results to a pandas DataFrame \n",
    "  *  Then display results.\n",
    "\n",
    "2. %%bigquery my_pandas_data_frame\n",
    "\n",
    "  *  Behind the scenes, the %%bigquery magic command uses the BigQuery client library for Python to run the given query\n",
    "  *  Then convert the results to a pandas DataFrame \n",
    "  *  Then save the pandas DataFrame to the variable my_pandas_data_frame\n",
    "  \n",
    "See exampe of these two \"cell magics\" below\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/workbench/instances/bigquery#query_data_by_using_the_bigquery_magic_command\n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/interactive/magics.html#cell-magics\n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/interactive/magics.html#line-magics\n",
    "\n",
    "https://cloud.google.com/python/docs/reference/bigquery/latest/magics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40035527-b288-4384-a072-1fa7bb7d569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "  country_code,\n",
    "  country_name,\n",
    "  COUNT(DISTINCT region_code) AS num_regions\n",
    "FROM\n",
    "  `bigquery-public-data.google_trends.international_top_terms`\n",
    "WHERE\n",
    "  refresh_date = DATE_SUB(CURRENT_DATE, INTERVAL 1 DAY)\n",
    "GROUP BY\n",
    "  country_code,\n",
    "  country_name\n",
    "ORDER BY\n",
    "  num_regions DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623f0cd6-a05f-412f-bdc2-5f0afcee7b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery regions_by_country\n",
    "SELECT\n",
    "  country_code,\n",
    "  country_name,\n",
    "  COUNT(DISTINCT region_code) AS num_regions\n",
    "FROM\n",
    "  `bigquery-public-data.google_trends.international_top_terms`\n",
    "WHERE\n",
    "  refresh_date = DATE_SUB(CURRENT_DATE, INTERVAL 1 DAY)\n",
    "GROUP BY\n",
    "  country_code, country_name\n",
    "ORDER BY\n",
    "  num_regions DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37135f-29e9-47a4-bb1b-a1d4eef75910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regions_by_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd7abff-6c1c-4a6b-987b-c0831798ec71",
   "metadata": {},
   "source": [
    "# Method 3 : Using the BigQuery python client library \n",
    "\n",
    "This is Method 3 of querying data that is stored in BigQuery from within the JupyterLab interface of your Vertex AI Workbench instance.\n",
    "\n",
    "For this method, it is required to use the IPython Jupyter Kernel that comes prepackaged with Vertex AI Workbench Instances. Please select & connect to it now.\n",
    "\n",
    "As per the example below, using the BigQuery client library involves: \n",
    "\n",
    "1. importing the library\n",
    "2. initialing a BigQuery client object\n",
    "3. defining your SQL query\n",
    "4. getting the BigQuery client object to execute SQL and return a pandas Dataframe\n",
    "5. with the returned pandas Dataframe you can use a number of Python data analysis, data wrangling, and visualization libraries\n",
    "\n",
    "\n",
    "\n",
    "Reference :\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/workbench/instances/bigquery#query_data_by_using_the_client_library_directly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44edb7-75bd-4392-8191-aa3dda5f0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e850be-7740-424e-be81-e5191f61ad7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "WITH\n",
    "  TopTermsByDate AS (\n",
    "    SELECT DISTINCT refresh_date AS date, term\n",
    "    FROM `bigquery-public-data.google_trends.top_terms`\n",
    "  ),\n",
    "  DistinctDates AS (\n",
    "    SELECT DISTINCT date\n",
    "    FROM TopTermsByDate\n",
    "  )\n",
    "SELECT\n",
    "  DATE_DIFF(Dates2.date, Date1Terms.date, DAY)\n",
    "    AS days_apart,\n",
    "  COUNT(DISTINCT (Dates2.date || Date1Terms.date))\n",
    "    AS num_date_pairs,\n",
    "  COUNT(Date1Terms.term) AS num_date1_terms,\n",
    "  SUM(IF(Date2Terms.term IS NOT NULL, 1, 0))\n",
    "    AS overlap_terms,\n",
    "  SAFE_DIVIDE(\n",
    "    SUM(IF(Date2Terms.term IS NOT NULL, 1, 0)),\n",
    "    COUNT(Date1Terms.term)\n",
    "    ) AS pct_overlap_terms\n",
    "FROM\n",
    "  TopTermsByDate AS Date1Terms\n",
    "CROSS JOIN\n",
    "  DistinctDates AS Dates2\n",
    "LEFT JOIN\n",
    "  TopTermsByDate AS Date2Terms\n",
    "  ON\n",
    "    Dates2.date = Date2Terms.date\n",
    "    AND Date1Terms.term = Date2Terms.term\n",
    "WHERE\n",
    "  Date1Terms.date <= Dates2.date\n",
    "GROUP BY\n",
    "  days_apart\n",
    "\n",
    "ORDER BY\n",
    "  days_apart;\n",
    "\"\"\"\n",
    "pct_overlap_terms_by_days_apart = client.query(sql).to_dataframe()\n",
    "\n",
    "pct_overlap_terms_by_days_apart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c563e2b-ef6b-4882-bed8-feb9691ccdc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Method 4 : Using the BigQuery integration into Vertex AI workbench\n",
    "\n",
    "This is Method 4 of querying data that is stored in BigQuery from within the JupyterLab interface of your Vertex AI Workbench instance.\n",
    "\n",
    "The In-cell BigQuery query editor is a cell type that you can use within your notebook files.\n",
    "\n",
    "Select the BigQuery icon on a given cell to launc the In-cell BigQuery query editor. \n",
    "\n",
    "Reference : \n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/workbench/instances/bigquery#in-cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b287a-f8a6-4d27-8a09-302026117973",
   "metadata": {},
   "source": [
    "# Method 5 : Use BigQuery DataFrames aka \"BigFrames\" to super charge pandas workloads\n",
    "\n",
    "This is Method 6 of querying data that is stored in BigQuery from within the JupyterLab interface of your Vertex AI Workbench instance.\n",
    "\n",
    "For this method, it is required to use the IPython Jupyter Kernel that comes prepackaged with Vertex AI Workbench Instances. Please select & connect to it now.\n",
    "\n",
    "BigQuery DataFrames provides a Python library **bigframes.pandas** which provides a pandas-compatible API for analytics. \n",
    "\n",
    "Through this library you take advantage of BigQuery data processing by using familiar pandas python APIs. BigQuery DataFrames provides a Pythonic DataFrame powered by the BigQuery engine, and it implements the pandas  APIs by pushing the processing down to BigQuery through SQL conversion. This lets you use BigQuery to explore and process terabytes of data with Python APIs.\n",
    "\n",
    "\n",
    "Reference : \n",
    "\n",
    "https://cloud.google.com/bigquery/docs/bigquery-dataframes-introduction\n",
    "\n",
    "https://cloud.google.com/bigquery/docs/use-bigquery-dataframes\n",
    "\n",
    "https://github.com/googleapis/python-bigquery-dataframes/tree/main/notebooks\n",
    "\n",
    "https://cloud.google.com/deep-learning-vm/docs/introduction#pre-installed_packages\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/workbench/instances/add-environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b9b28-d1c5-4116-bdb7-fe00892ca759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2de67a6a-152e-4d1a-a79e-9d994ae0d62c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Method 6 : Use BigQuery DataFrames aka \"BigFrames\" to super charge scikit learn workloads\n",
    "\n",
    "This is Method 7 of querying data that is stored in BigQuery from within the JupyterLab interface of your Vertex AI Workbench instance.\n",
    "\n",
    "For this method, it is required to use the IPython Jupyter Kernel that comes prepackaged with Vertex AI Workbench Instances. Please select & connect to it now.\n",
    "\n",
    "BigQuery DataFrames provides a library **bigframes.ml** which provides a scikit-learn-like API for machine learning (ML). \n",
    "\n",
    "Through this library you take advantage of BigQuery data processing by using familiar Python scikit-learn-like APIs. BigQuery DataFrames provides a Pythonic DataFrame powered by the BigQuery engine, and it implements the scikit-learn APIs by pushing the processing down to BigQuery through SQL conversion (specifically BigQuery ML). This lets you use BigQuery to explore and process terabytes of data, and also train machine learning (ML) models, all with Python APIs.\n",
    "\n",
    "Reference : \n",
    "\n",
    "https://cloud.google.com/bigquery/docs/bigquery-dataframes-introduction\n",
    "\n",
    "https://cloud.google.com/bigquery/docs/use-bigquery-dataframes\n",
    "\n",
    "https://github.com/googleapis/python-bigquery-dataframes/tree/main/notebooks\n",
    "\n",
    "https://cloud.google.com/bigquery/docs/bqml-introduction\n",
    "\n",
    "https://github.com/googleapis/python-bigquery-dataframes/blob/main/notebooks/getting_started/ml_fundamentals_bq_dataframes.ipynb\n",
    "\n",
    "https://cloud.google.com/deep-learning-vm/docs/introduction#pre-installed_packages\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/workbench/instances/add-environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a9018a-2f9b-48e0-9af6-5dfa68307c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install bigframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95764d70-918d-46ea-a13b-c7edf06a8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigframes.pandas as bpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee648c5b-ebab-491c-a8a2-4ab62254146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bpd.read_gbq(\"bigquery-public-data.ml_datasets.penguins\")\n",
    "df = df.dropna()\n",
    "\n",
    "# BigQuery DataFrames creates a default numbered index, which we can give a name\n",
    "df.index.name = \"penguin_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b08bcd2-65eb-4294-850a-7d4a5870798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate input features and output variable into DataFrames\n",
    "X = df[['island', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'sex', 'species']]\n",
    "y = df[['body_mass_g']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d1a171-e364-477d-ac86-63abcadd317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigframes.ml.model_selection import train_test_split\n",
    "\n",
    "# This will split X and y into test and training sets, with 20% of the rows in the test set,\n",
    "# and the rest in the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2)\n",
    "\n",
    "# Show the shape of the data after the split\n",
    "print(f\"\"\"X_train shape: {X_train.shape}\n",
    "X_test shape: {X_test.shape}\n",
    "y_train shape: {y_train.shape}\n",
    "y_test shape: {y_test.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae703f-8c8c-4623-9ec9-dda478245fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigframes.ml.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler will only work on numeric columns\n",
    "numeric_columns = [\"culmen_length_mm\", \"culmen_depth_mm\", \"flipper_length_mm\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[numeric_columns])\n",
    "\n",
    "# Now, standardscaler should transform the numbers to have mean of zero\n",
    "# and standard deviation of one:\n",
    "scaler.transform(X_train[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a7357-374e-41c6-aeea-5fb619af599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(X_test[numeric_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af34b2-fa0b-4cd8-9b0d-3ab3cf462041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigframes.ml.compose import ColumnTransformer\n",
    "from bigframes.ml.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create an aggregate transform that applies StandardScaler to the numeric columns,\n",
    "# and OneHotEncoder to the string columns\n",
    "preproc = ColumnTransformer([\n",
    "    (\"scale\", StandardScaler(), [\"culmen_length_mm\", \"culmen_depth_mm\", \"flipper_length_mm\"]),\n",
    "    (\"encode\", OneHotEncoder(), [\"species\", \"sex\", \"island\"])])\n",
    "\n",
    "# Now we can fit all columns of the training data\n",
    "preproc.fit(X_train)\n",
    "\n",
    "processed_X_train = preproc.transform(X_train)\n",
    "processed_X_test = preproc.transform(X_test)\n",
    "\n",
    "# View the processed training data\n",
    "processed_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53914e-f9d8-4e4c-90c6-e40289a6cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigframes.ml.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Learn from the training data how to predict output y\n",
    "linreg.fit(processed_X_train, y_train)\n",
    "\n",
    "# Predict y for the test data\n",
    "predicted_y_test = linreg.predict(processed_X_test)\n",
    "\n",
    "# View predictions\n",
    "predicted_y_test"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
